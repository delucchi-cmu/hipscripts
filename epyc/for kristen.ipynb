{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d18ed4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "import lsdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import scipy\n",
    "import astropy.units as u\n",
    "import warnings\n",
    "\n",
    "from dask.distributed import Client\n",
    "from nested_pandas.utils import count_nested\n",
    "from dask.distributed import print as dask_print\n",
    "from lsdb.core.search.pixel_search import PixelSearch\n",
    "from lsdb.core.search.order_search import OrderSearch\n",
    "from lsdb.core.search import ConeSearch\n",
    "from io import StringIO\n",
    "from nested_pandas import NestedDtype\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f55d81",
   "metadata": {},
   "source": [
    "These are just the 6 fields of LSST commissioning (I wanted a list of a few cone searches, and this is the first one I thought of).\n",
    "\n",
    "First, we can just look at fetching GAIA in a (smaller) cone in each field.\n",
    "\n",
    "Alternatively, you can just fetch the full 1.1TB, following directions on https://data.lsdb.io/#Gaia/Gaia_DR3_(GAIA_SOURCE)\n",
    "\n",
    "```\n",
    "wget -r -np -nH --cut-dirs=2 -R \"*.html*\" https://data.lsdb.io/hats/gaia_dr3/gaia/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3037c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {\n",
    "    \"ECDFS\": (53.13, -28.10),  # Extended Chandra Deep Field South\n",
    "    \"EDFS\": (59.10, -48.73),  # Euclid Deep Field South\n",
    "    \"Rubin_SV_38_7\": (37.86, 6.98),  # Low Ecliptic Latitude Field\n",
    "    \"Rubin_SV_95_-25\": (95.00, -25.00),  # Low Galactic Latitude Field\n",
    "    \"47_Tuc\": (6.02, -72.08),  # 47 Tuc Globular Cluster\n",
    "    \"Fornax_dSph\": (40.00, -34.45),  # Fornax Dwarf Spheroidal Galaxy\n",
    "}\n",
    "# Define the radius for selecting sources\n",
    "selection_radius_arcsec = 0.5 * 3600  # 1/2-degree radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e3cfd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaia_full = lsdb.read_hats(\"https://data.lsdb.io/hats/gaia_dr3/gaia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02223129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cone for  ECDFS\n",
      "   has 2795 rows\n",
      "cone for  EDFS\n",
      "   has 3249 rows\n",
      "cone for  Rubin_SV_38_7\n",
      "   has 2522 rows\n",
      "cone for  Rubin_SV_95_-25\n",
      "   has 15620 rows\n",
      "cone for  47_Tuc\n",
      "   has 157203 rows\n",
      "cone for  Fornax_dSph\n",
      "   has 47906 rows\n"
     ]
    }
   ],
   "source": [
    "for field_name, field_region in fields.items():\n",
    "    print(\"cone for \", field_name)\n",
    "    cone = ConeSearch(\n",
    "        ra=field_region[0],\n",
    "        dec=field_region[1],\n",
    "        radius_arcsec=selection_radius_arcsec,\n",
    "    )\n",
    "    \n",
    "    ## restrict the catalog to just the cone we're interested in\n",
    "    ## this DOESN'T change the `gaia_full` object, so we can re-use it.\n",
    "    ## this operation is also \"lazy\"\n",
    "    gaia_partial = gaia_full.search(cone)\n",
    "    \n",
    "    ## this is the \"eager\" call, that will return a pandas dataframe with\n",
    "    ## just the results of the cone search. what you do with it is up to you!\n",
    "    cone_results = gaia_partial.compute()\n",
    "    cone_results.to_parquet(f\"gaia_{field_name}.pq\")\n",
    "    print(\"   has\", len(cone_results), \"rows\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9295ecfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30M\tgaia_47_Tuc.pq\r\n",
      "596K\tgaia_ECDFS.pq\r\n",
      "704K\tgaia_EDFS.pq\r\n",
      "8.9M\tgaia_Fornax_dSph.pq\r\n",
      "548K\tgaia_Rubin_SV_38_7.pq\r\n",
      "3.3M\tgaia_Rubin_SV_95_-25.pq\r\n",
      "4.0K\tgaia_new.py\r\n",
      "44M\ttotal\r\n"
     ]
    }
   ],
   "source": [
    "!du -ch gaia_*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723f0d5",
   "metadata": {},
   "source": [
    "You could repeat that process for all the catalogs that you're interested in that we offer through data.lsdb.io:\n",
    "\n",
    "* ZTF: https://data.lsdb.io/#ZTF/ZTF_DR22\n",
    "* eRosita: https://data.lsdb.io/#eRASS1_Main\n",
    "\n",
    "I'd not heard of EMU or asas-sn before this. If they're public data sets, and have discrete releases, we can potentially HATS-ify them and host them (but that takes around a month for us to get around to it, assuming we can find the data).\n",
    "\n",
    "If all of your data is in HATS (or in a pandas dataframe that can be HATS-ified (see [this notebook](https://docs.lsdb.io/en/stable/tutorials/pre_executed/ztf_bts-ngc.html#Put-both-catalogs-to-LSDB-and-plan-cross-match))), then you can do everything within LSDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd3ee135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet nway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b744a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsdb.core.crossmatch.abstract_crossmatch_algorithm import AbstractCrossmatchAlgorithm\n",
    "import nwaylib.fastskymatch as match\n",
    "import hats.pixel_math.healpix_shim as hp\n",
    "\n",
    "class NWAYCrossmatch(AbstractCrossmatchAlgorithm):\n",
    "    \n",
    "    @classmethod\n",
    "    def validate(match_radius):\n",
    "        \"\"\"This should really be implemented to check that we've provided valid arguments\"\"\"\n",
    "        if match_radius < 0:\n",
    "            raise ValueError(\"match radius has to be positive, you silly goose.\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def perform_crossmatch(\n",
    "            self,\n",
    "            match_radius,\n",
    "        ):\n",
    "    \n",
    "        ## Looks like they want astropy Tables. The per-partition data\n",
    "        ## comes in as pandas dataframes, so let's just convert them to tables.\n",
    "        tables = [Table.from_pandas(self.left), Table.from_pandas(self.right)]\n",
    "        \n",
    "        ## The catalog names are stashed in this member of AbstractCrossmatchAlgorithm\n",
    "        table_names = self.suffixes\n",
    "        \n",
    "        \n",
    "        fits_formats = None\n",
    "        \n",
    "        ## I can't figure out where source_densities go, but since these are \n",
    "        ## healpix tiles, you can find the area real-easily.\n",
    "        \n",
    "        area_total = (4 * pi * (180 / pi)**2)\n",
    "        \n",
    "        left_area = hp.order2pixarea(self.left_order)\n",
    "        left_density = len(self.left) / left_area * area_total\n",
    "        right_area = hp.order2pixarea(self.right_order)\n",
    "        right_density = len(self.right) / right_area * area_total\n",
    "        \n",
    "        # Then you call it, and convert it back to a dataframe.    \n",
    "        results, columns, match_header = match.match_multiple(tables, \n",
    "                                                              table_names, \n",
    "                                                              match_radius, \n",
    "                                                              fits_formats)\n",
    "        \n",
    "        ## you're going to need to do more here, but I'm not sure what right now.\n",
    "        return results.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb1673",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can reduce your search space in gaiaXerosita by first matching to the clusters you know about.\n",
    "## This one doesn't specify the crossmatch algorithm because nearest neighbor is \"ok enough\".\n",
    "cluster_gaia = gaia_full.crossmatch(cluster_catalog)\n",
    "\n",
    "## Then crossmatch that area with xrays from erosita.\n",
    "xray_counterparts = cluster_gaia.crossmatch(erosita_catalog, algorithm=NWAYCrossmatch).compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeLucchi HATS",
   "language": "python",
   "name": "hatsenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
