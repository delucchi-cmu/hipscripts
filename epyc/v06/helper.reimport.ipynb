{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a0a8f5-fd7e-463d-9798-d3f49c3e8c9b",
   "metadata": {},
   "source": [
    "# Re-import helper\n",
    "\n",
    "Concatenation of several helper methods to determine the appropriate arguments for importing a dataset that has previously been imported into HATS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc0c01a-8dd0-43fc-85be-154e0042633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hats_import\n",
    "import hats\n",
    "from hats.pixel_math import HealpixPixel\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import hats\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "hats_import.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a048bc1e-25ed-4bda-b984-0bf368807a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change this path!!!\n",
    "catalog_dir = \"/epyc/data3/hats/catalogs/tic\"\n",
    "\n",
    "### ----------------\n",
    "### You probably won't have to change anything from here.\n",
    "\n",
    "catalog = hats.read_hats(catalog_dir)\n",
    "\n",
    "info_frame = catalog.partition_info.as_dataframe()\n",
    "\n",
    "for index, partition in info_frame.iterrows():\n",
    "    file_name = result = hats.io.paths.pixel_catalog_file(\n",
    "        catalog_dir, HealpixPixel(partition[\"Norder\"], partition[\"Npix\"])\n",
    "    )\n",
    "    info_frame.loc[index, \"size_on_disk\"] = os.path.getsize(file_name)\n",
    "\n",
    "info_frame = info_frame.astype(int)\n",
    "info_frame[\"gbs\"] = info_frame[\"size_on_disk\"] / (1024 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafb416f-0bca-4fc4-abb5-20d5de561be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'healpix orders: {info_frame[\"Norder\"].unique()}')\n",
    "print(f'num partitions: {len(info_frame[\"Npix\"])}')\n",
    "print(\"------\")\n",
    "print(f'min size_on_disk: {info_frame[\"gbs\"].min():.2f}')\n",
    "print(f'max size_on_disk: {info_frame[\"gbs\"].max():.2f}')\n",
    "print(f'size_on_disk ratio: {info_frame[\"gbs\"].max()/info_frame[\"gbs\"].min():.2f}')\n",
    "print(f'total size_on_disk: {info_frame[\"gbs\"].sum():.2f}')\n",
    "print(\"------\")\n",
    "\n",
    "plt.hist(info_frame[\"gbs\"])\n",
    "\n",
    "bins = [0, 0.5, 1, 2, 100]\n",
    "labels = [\"small-ish\", \"sweet-spot\", \"big-ish\", \"too-big\"]\n",
    "hist = np.histogram(info_frame[\"gbs\"], bins=bins)[0]\n",
    "pcts = hist / len(info_frame)\n",
    "for i in range(0, len(labels)):\n",
    "    print(f\"{labels[i]} \\t: {hist[i]} \\t({pcts[i]*100:.1f} %)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed0c3d-821b-4843-9fb1-069fb625b8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra_column = catalog.catalog_info.ra_column\n",
    "stats = catalog.per_pixel_statistics(include_columns=[ra_column], include_stats=[\"row_count\"])\n",
    "biggest_parts = stats.sort_values([f\"{ra_column}: row_count\"], ascending=False).head(5)\n",
    "print(catalog.catalog_info)\n",
    "\n",
    "biggest_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fdd02e-c7bd-4a7a-91f8-a1b5b2b334f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_pixel = biggest_parts.index[0]\n",
    "sample_parquet_file = f\"{catalog_dir}/dataset/Norder={biggest_pixel.order}/Dir={biggest_pixel.dir}/Npix={biggest_pixel.pixel}.parquet\"\n",
    "\n",
    "sample_file_size = os.path.getsize(sample_parquet_file)\n",
    "parquet_file = pq.ParquetFile(sample_parquet_file)\n",
    "num_rows = parquet_file.metadata.num_rows\n",
    "\n",
    "## 300MB\n",
    "ideal_file_small = 300 * 1024 * 1024\n",
    "## 1G\n",
    "ideal_file_large = 1024 * 1024 * 1024\n",
    "\n",
    "threshold_small = ideal_file_small / sample_file_size * num_rows\n",
    "threshold_large = ideal_file_large / sample_file_size * num_rows\n",
    "\n",
    "\n",
    "print(f\"ra_column='{catalog.catalog_info.ra_column}',\")\n",
    "print(f\"dec_column='{catalog.catalog_info.dec_column}',\")\n",
    "print(f\"expected_total_rows={int(catalog.catalog_info.total_rows):_},\")\n",
    "print(f\"pixel_threshold= BETWEEN {int(threshold_small):_} AND {int(threshold_large):_},\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad617860-7c75-4ea5-aa13-fa351446b548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeLucchi HATS v07",
   "language": "python",
   "name": "delucchi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
