{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03c41b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from importlib.metadata import version\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.dataset as pds\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from hipscat.io import paths\n",
    "\n",
    "PROVENANCE_INFO_FILENAME = \"provenance_info.json\"\n",
    "PARQUET_METADATA_FILENAME = \"_metadata\"\n",
    "PARQUET_COMMON_METADATA_FILENAME = \"_common_metadata\"\n",
    "\n",
    "\n",
    "def write_parquet_metadata(catalog_path):\n",
    "    \"\"\"Generate parquet metadata, using the already-partitioned parquet files\n",
    "    for this catalog\n",
    "    Args:\n",
    "        catalog_path (str): base path for the catalog\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = pds.dataset(catalog_path,\n",
    "                          partitioning=\"hive\", \n",
    "                          format=\"parquet\", \n",
    "                          exclude_invalid_files=True)\n",
    "    metadata_collector = []\n",
    "    all_names = set()\n",
    "\n",
    "    for hips_file in dataset.files:\n",
    "        ## Get rid of any non-parquet files\n",
    "        if not hips_file.endswith(\"parquet\"):\n",
    "            continue\n",
    "        single_metadata = pq.read_metadata(hips_file)\n",
    "        metadata_collector.append(single_metadata)\n",
    "        all_names.update(single_metadata.schema.to_arrow_schema().names)\n",
    "    print(all_names)\n",
    "    dataset_names = set(dataset.schema.names)\n",
    "    print(all_names.difference(dataset_names))\n",
    "    hive_names = dataset_names.difference(all_names)\n",
    "    known_hive_names = set([paths.ORDER_DIRECTORY_PREFIX, paths.DIR_DIRECTORY_PREFIX, \"parts\"])\n",
    "    unknown_names = hive_names.difference(known_hive_names)\n",
    "    print(unknown_names)\n",
    "\n",
    "    ## Trim hive fields from final schema, otherwise there will be a mismatch.\n",
    "    subschema = dataset.schema\n",
    "    hive_fields = [paths.ORDER_DIRECTORY_PREFIX, paths.DIR_DIRECTORY_PREFIX, \"parts\"]\n",
    "    for hive_field in hive_fields:\n",
    "        field_index = subschema.get_field_index(hive_field)\n",
    "        if field_index != -1:\n",
    "            subschema = subschema.remove(field_index)\n",
    "\n",
    "    metadata_path = os.path.join(catalog_path, PARQUET_METADATA_FILENAME)\n",
    "    common_metadata_path = os.path.join(catalog_path, PARQUET_COMMON_METADATA_FILENAME)\n",
    "\n",
    "#     pq.write_metadata(subschema, metadata_path, metadata_collector=metadata_collector)\n",
    "#     pq.write_metadata(subschema, common_metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f042e4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'catflags', 'band', 'fieldID', '_hipscat_id', 'rcID', 'mjd', 'index', 'ps1_objid', 'mag', '__index_level_0__', 'dec', 'ra', 'maggerr'}\n",
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "# tables = [\"object\", \"source\", \"object_to_source\"]\n",
    "\n",
    "# for table in tables:\n",
    "#     write_parquet_metadata(f\"/data3/epyc/data3/hipscat/catalogs/ztf_mar16/{table}/\")\n",
    "    \n",
    "# tables = [\"object_index\", \"source_index\"]\n",
    "\n",
    "# for table in tables:\n",
    "#     write_parquet_metadata(f\"/data3/epyc/data3/hipscat/catalogs/ztf_mar16/{table}/\")\n",
    "\n",
    "write_parquet_metadata(f\"/data3/epyc/data3/hipscat/catalogs/ztf_mar16/source/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f920684",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pds.dataset(f\"/data3/epyc/data3/hipscat/catalogs/ztf_mar16/object_index/\", \n",
    "                      partitioning=\"hive\", \n",
    "                      format=\"parquet\", \n",
    "                      exclude_invalid_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fa35c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index',\n",
       " '_hipscat_id',\n",
       " 'ps1_objid',\n",
       " 'ra',\n",
       " 'dec',\n",
       " 'ps1_gMeanPSFMag',\n",
       " 'ps1_rMeanPSFMag',\n",
       " 'ps1_iMeanPSFMag',\n",
       " 'nobs_g',\n",
       " 'nobs_r',\n",
       " 'nobs_i',\n",
       " 'mean_mag_g',\n",
       " 'mean_mag_r',\n",
       " 'mean_mag_i',\n",
       " '__index_level_0__']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# md = pq.read_metadata(\"/data3/epyc/data3/hipscat/catalogs/ztf_mar16/object_index/parts=0/part_000_of_001.parquet\")\n",
    "md = pq.read_metadata(\"/data3/epyc/data3/hipscat/catalogs/ztf_mar16/object/Norder=1/Dir=0/Npix=33.parquet\")\n",
    "md.schema.to_arrow_schema().names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Melissa LSDB",
   "language": "python",
   "name": "lsd2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
