{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98af180d",
   "metadata": {},
   "source": [
    "# Estimate pixel threshold\n",
    "\n",
    "For best performance, we try to keep catalog parquet files between 200-800MB in size.\n",
    "\n",
    "**Background**\n",
    "\n",
    "When creating a new catalog through the hipscat-import process, we try to create partitions with approximately the same number of rows per partition. This isn't perfect, because the sky is uneven, but we still try to create smaller-area pixels in more dense areas, and larger-area pixels in less dense areas. We use the argument `pixel_threshold` and will split a partition into smaller healpix pixels until the number of rows is smaller than `pixel_threshold`.\n",
    "\n",
    "We do this to increase parallelization of reads and downstream analysis: if the files are around the same size, and operations on each partition take around the same amount of time, we're not as likely to be waiting on a single process to complete for the whole pipeline to complete.\n",
    "\n",
    "In addition, a single catalog file should not exceed a couple GB - we're going to need to read the whole thing into memory, so it needs to fit!\n",
    "\n",
    "**Objective**\n",
    "\n",
    "In this notebook, we'll go over *one* strategy for estimating the `pixel_threshold` argument you can use when importing a new catalog into hipscat format.\n",
    "\n",
    "This is not guaranteed to give you optimal results, but it could give you some hints toward *better* results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb86458c",
   "metadata": {},
   "source": [
    "## Create a sample parquet file\n",
    "\n",
    "The first step is to read in your survey data in its original form, and convert a sample into parquet. This has a few benefits:\n",
    "- parquet uses compression in various ways, and by creating the sample, we can get a sense of both the overall and field-level compression with real dat\n",
    "- using the importer `FileReader` interface now sets you up for more success when you get around to importing!\n",
    "\n",
    "If your data is already in parquet format, just change the `sample_parquet_file` path to an existing file, and don't run the second cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5dd94480",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_parquet_file=\"/data3/epyc/data3/hipscat/raw/sdss/parquet/calibObj-008162-4-star.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e6a53db0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot convert a table with multidimensional columns to a pandas DataFrame. Offending columns are: ['ROWC', 'COLC', 'M_RR_CC', 'M_RR_CC_PSF', 'FLAGS', 'FLAGS2', 'PSP_STATUS', 'PSF_FWHM', 'EXTINCTION', 'SKYFLUX', 'PSFFLUX', 'PSFFLUX_IVAR', 'FIBERFLUX', 'FIBERFLUX_IVAR', 'FIBER2FLUX', 'FIBER2FLUX_IVAR', 'MODELFLUX', 'MODELFLUX_IVAR', 'CALIB_STATUS', 'NMGYPERCOUNT', 'APERFLUX6', 'CMODELFLUX_CLEAN', 'CMODELFLUX_CLEAN_IVAR', 'CMODELFLUX_CLEAN_VAR', 'CMODELFLUX_CLEAN_CHI2', 'CMODEL_CLEAN_NUSE', 'CMODEL_CLEAN_MJD_MAXDIFF', 'CMODEL_CLEAN_MJD_VAR', 'MODELFLUX_CLEAN', 'MODELFLUX_CLEAN_IVAR', 'MODELFLUX_CLEAN_VAR', 'MODELFLUX_CLEAN_CHI2', 'MODEL_CLEAN_NUSE', 'MODEL_CLEAN_MJD_MAXDIFF', 'MODEL_CLEAN_MJD_VAR', 'PSFFLUX_CLEAN', 'PSFFLUX_CLEAN_IVAR', 'PSFFLUX_CLEAN_VAR', 'PSFFLUX_CLEAN_CHI2', 'PSF_CLEAN_NUSE', 'PSF_CLEAN_MJD_MAXDIFF', 'PSF_CLEAN_MJD_VAR']\nOne can filter out such columns using:\nnames = [name for name in tbl.colnames if len(tbl[name].shape) <= 1]\ntbl[names].to_pandas(...)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 58\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m## This input CSV file requires header and type data from another source.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m file_reader\u001b[38;5;241m=\u001b[39mFitsReader(\n\u001b[1;32m     55\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50_000\u001b[39m,\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#     skip_column_names=MULTIDIMENSIONAL,\u001b[39;00m\n\u001b[1;32m     57\u001b[0m )\n\u001b[0;32m---> 58\u001b[0m data_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m data_frame\u001b[38;5;241m.\u001b[39mto_parquet(sample_parquet_file)\n",
      "File \u001b[0;32m~/git/hipscat-import/src/hipscat_import/catalog/file_readers.py:220\u001b[0m, in \u001b[0;36mread\u001b[0;34m(self, input_file)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500_000\u001b[39m, column_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, skip_column_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    219\u001b[0m ):\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunksize \u001b[38;5;241m=\u001b[39m chunksize\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_names \u001b[38;5;241m=\u001b[39m column_names\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_column_names \u001b[38;5;241m=\u001b[39m skip_column_names\n",
      "File \u001b[0;32m~/anaconda3/envs/lsd2env/lib/python3.9/site-packages/astropy/table/table.py:3713\u001b[0m, in \u001b[0;36mTable.to_pandas\u001b[0;34m(self, index, use_nullable_int)\u001b[0m\n\u001b[1;32m   3711\u001b[0m badcols \u001b[38;5;241m=\u001b[39m [name \u001b[38;5;28;01mfor\u001b[39;00m name, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(col\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m badcols:\n\u001b[0;32m-> 3713\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3714\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot convert a table with multidimensional columns to a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   3715\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpandas DataFrame. Offending columns are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbadcols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   3716\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOne can filter out such columns using:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   3717\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnames = [name for name in tbl.colnames if len(tbl[name].shape) <= 1]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   3718\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtbl[names].to_pandas(...)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3720\u001b[0m out \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   3722\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, column \u001b[38;5;129;01min\u001b[39;00m tbl\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot convert a table with multidimensional columns to a pandas DataFrame. Offending columns are: ['ROWC', 'COLC', 'M_RR_CC', 'M_RR_CC_PSF', 'FLAGS', 'FLAGS2', 'PSP_STATUS', 'PSF_FWHM', 'EXTINCTION', 'SKYFLUX', 'PSFFLUX', 'PSFFLUX_IVAR', 'FIBERFLUX', 'FIBERFLUX_IVAR', 'FIBER2FLUX', 'FIBER2FLUX_IVAR', 'MODELFLUX', 'MODELFLUX_IVAR', 'CALIB_STATUS', 'NMGYPERCOUNT', 'APERFLUX6', 'CMODELFLUX_CLEAN', 'CMODELFLUX_CLEAN_IVAR', 'CMODELFLUX_CLEAN_VAR', 'CMODELFLUX_CLEAN_CHI2', 'CMODEL_CLEAN_NUSE', 'CMODEL_CLEAN_MJD_MAXDIFF', 'CMODEL_CLEAN_MJD_VAR', 'MODELFLUX_CLEAN', 'MODELFLUX_CLEAN_IVAR', 'MODELFLUX_CLEAN_VAR', 'MODELFLUX_CLEAN_CHI2', 'MODEL_CLEAN_NUSE', 'MODEL_CLEAN_MJD_MAXDIFF', 'MODEL_CLEAN_MJD_VAR', 'PSFFLUX_CLEAN', 'PSFFLUX_CLEAN_IVAR', 'PSFFLUX_CLEAN_VAR', 'PSFFLUX_CLEAN_CHI2', 'PSF_CLEAN_NUSE', 'PSF_CLEAN_MJD_MAXDIFF', 'PSF_CLEAN_MJD_VAR']\nOne can filter out such columns using:\nnames = [name for name in tbl.colnames if len(tbl[name].shape) <= 1]\ntbl[names].to_pandas(...)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from hipscat_import.catalog.file_readers import FitsReader\n",
    "\n",
    "\n",
    "MULTIDIMENSIONAL = [\n",
    "    \"ROWC\",\n",
    "    \"COLC\",\n",
    "    \"M_RR_CC\",\n",
    "    \"M_RR_CC_PSF\",\n",
    "    \"FLAGS\",\n",
    "    \"FLAGS2\",\n",
    "    \"PSP_STATUS\",\n",
    "    \"PSF_FWHM\",\n",
    "    \"EXTINCTION\",\n",
    "    \"SKYFLUX\",\n",
    "    \"PSFFLUX\",\n",
    "    \"PSFFLUX_IVAR\",\n",
    "    \"FIBERFLUX\",\n",
    "    \"FIBERFLUX_IVAR\",\n",
    "    \"FIBER2FLUX\",\n",
    "    \"FIBER2FLUX_IVAR\",\n",
    "    \"MODELFLUX\",\n",
    "    \"MODELFLUX_IVAR\",\n",
    "    \"CALIB_STATUS\",\n",
    "    \"NMGYPERCOUNT\",\n",
    "    \"APERFLUX6\",\n",
    "    \"CMODELFLUX_CLEAN\",\n",
    "    \"CMODELFLUX_CLEAN_IVAR\",\n",
    "    \"CMODELFLUX_CLEAN_VAR\",\n",
    "    \"CMODELFLUX_CLEAN_CHI2\",\n",
    "    \"CMODEL_CLEAN_NUSE\",\n",
    "    \"CMODEL_CLEAN_MJD_MAXDIFF\",\n",
    "    \"CMODEL_CLEAN_MJD_VAR\",\n",
    "    \"MODELFLUX_CLEAN\",\n",
    "    \"MODELFLUX_CLEAN_IVAR\",\n",
    "    \"MODELFLUX_CLEAN_VAR\",\n",
    "    \"MODELFLUX_CLEAN_CHI2\",\n",
    "    \"MODEL_CLEAN_NUSE\",\n",
    "    \"MODEL_CLEAN_MJD_MAXDIFF\",\n",
    "    \"MODEL_CLEAN_MJD_VAR\",\n",
    "    \"PSFFLUX_CLEAN\",\n",
    "    \"PSFFLUX_CLEAN_IVAR\",\n",
    "    \"PSFFLUX_CLEAN_VAR\",\n",
    "    \"PSFFLUX_CLEAN_CHI2\",\n",
    "    \"PSF_CLEAN_NUSE\",\n",
    "    \"PSF_CLEAN_MJD_MAXDIFF\",\n",
    "    \"PSF_CLEAN_MJD_VAR\",\n",
    "]\n",
    "\n",
    "input_file=\"/data3/epyc/data3/hipscat/raw/sdss/calibObj-008162-4-star.fits.gz\"\n",
    "\n",
    "## This input CSV file requires header and type data from another source.\n",
    "\n",
    "file_reader=FitsReader(\n",
    "    chunksize=50_000,\n",
    "#     skip_column_names=MULTIDIMENSIONAL,\n",
    ")\n",
    "data_frame = next(file_reader.read(input_file))\n",
    "data_frame.to_parquet(sample_parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124eb444",
   "metadata": {},
   "source": [
    "## Inspect parquet file and metadata\n",
    "\n",
    "Now that we have parsed our survey data into parquet, we can check what the data will look like when it's imported into hipscat format.\n",
    "\n",
    "If you're just here to get a naive estimate for your pixel threshold, we'll do that first, then take a look at some other parquet characteristics later for the curious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a9f0e279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold between 150_144 and 1_201_153\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "sample_file_size = os.path.getsize(sample_parquet_file)\n",
    "parquet_file = pq.ParquetFile(sample_parquet_file)\n",
    "num_rows = parquet_file.metadata.num_rows\n",
    "\n",
    "## 100MB\n",
    "ideal_file_small = 100  *1024*1024\n",
    "## 800MB\n",
    "ideal_file_large = 800  *1024*1024\n",
    "\n",
    "threshold_small = ideal_file_small/sample_file_size*num_rows\n",
    "threshold_large = ideal_file_large/sample_file_size*num_rows\n",
    "\n",
    "print(f\"threshold between {int(threshold_small):_} and {int(threshold_large):_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23971c38",
   "metadata": {},
   "source": [
    "## Want to see more?\n",
    "\n",
    "I'm so glad you're still here! I have more to show you!\n",
    "\n",
    "The first step below shows us the file-level metadata, as stored by parquet. The number of columns here SHOULD match your expectations on the number of columns in your survey data.\n",
    "\n",
    "The `serialized_size` value is just the size of the total metadata, not the size of the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cc402acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow._parquet.FileMetaData object at 0x7f44ad38cd10>\n",
      "  created_by: parquet-cpp-arrow version 9.0.0\n",
      "  num_columns: 44\n",
      "  num_rows: 50000\n",
      "  num_row_groups: 1\n",
      "  format_version: 2.6\n",
      "  serialized_size: 20972\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "parquet_file = pq.ParquetFile(sample_parquet_file)\n",
    "print(parquet_file.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7835b6d9",
   "metadata": {},
   "source": [
    "The next step is to look at the column-level metadata. You can check that the on-disk type of each column matches your expectation of the data. Note that for some integer types, the on-disk type may be a smaller int than originally set (e.g. `bitWidth=8` or `16`). This is part of parquet's multi-part compression strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ff8fb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow._parquet.ParquetSchema object at 0x7f44ad182ec0>\n",
      "required group field_id=-1 schema {\n",
      "  optional int32 field_id=-1 RUN (Int(bitWidth=16, isSigned=true));\n",
      "  optional binary field_id=-1 RERUN;\n",
      "  optional int32 field_id=-1 CAMCOL (Int(bitWidth=8, isSigned=false));\n",
      "  optional int32 field_id=-1 FIELD (Int(bitWidth=16, isSigned=true));\n",
      "  optional int32 field_id=-1 ID (Int(bitWidth=16, isSigned=true));\n",
      "  optional int32 field_id=-1 OBJC_TYPE;\n",
      "  optional int32 field_id=-1 OBJC_FLAGS;\n",
      "  optional int32 field_id=-1 OBJC_FLAGS2;\n",
      "  optional float field_id=-1 OBJC_ROWC;\n",
      "  optional float field_id=-1 ROWVDEG;\n",
      "  optional float field_id=-1 ROWVDEGERR;\n",
      "  optional float field_id=-1 COLVDEG;\n",
      "  optional float field_id=-1 COLVDEGERR;\n",
      "  optional double field_id=-1 RA;\n",
      "  optional double field_id=-1 DEC;\n",
      "  optional int32 field_id=-1 RESOLVE_STATUS;\n",
      "  optional int32 field_id=-1 THING_ID;\n",
      "  optional int32 field_id=-1 IFIELD;\n",
      "  optional int32 field_id=-1 BALKAN_ID;\n",
      "  optional int32 field_id=-1 NDETECT;\n",
      "  optional int32 field_id=-1 PM_MATCH;\n",
      "  optional float field_id=-1 PMRA;\n",
      "  optional float field_id=-1 PMDEC;\n",
      "  optional float field_id=-1 PMRAERR;\n",
      "  optional float field_id=-1 PMDECERR;\n",
      "  optional float field_id=-1 PM_SIGRA;\n",
      "  optional float field_id=-1 PM_SIGDEC;\n",
      "  optional int32 field_id=-1 PM_NFIT;\n",
      "  optional float field_id=-1 PM_DIST22;\n",
      "  optional float field_id=-1 TMASS_J;\n",
      "  optional float field_id=-1 TMASS_J_IVAR;\n",
      "  optional float field_id=-1 TMASS_H;\n",
      "  optional float field_id=-1 TMASS_H_IVAR;\n",
      "  optional float field_id=-1 TMASS_K;\n",
      "  optional float field_id=-1 TMASS_K_IVAR;\n",
      "  optional binary field_id=-1 TMASS_PH_QUAL;\n",
      "  optional int32 field_id=-1 TMASS_RD_FLG (Int(bitWidth=16, isSigned=true));\n",
      "  optional int32 field_id=-1 TMASS_BL_FLG (Int(bitWidth=16, isSigned=true));\n",
      "  optional binary field_id=-1 TMASS_CC_FLG;\n",
      "  optional int32 field_id=-1 TMASS_GAL_CONTAM (Int(bitWidth=8, isSigned=false));\n",
      "  optional int32 field_id=-1 TMASS_MP_FLG (Int(bitWidth=8, isSigned=false));\n",
      "  optional binary field_id=-1 TMASS_HEMIS;\n",
      "  optional double field_id=-1 TMASS_JDATE;\n",
      "  optional int32 field_id=-1 ZHEDFLAG (Int(bitWidth=16, isSigned=true));\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(parquet_file.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b593b",
   "metadata": {},
   "source": [
    "Parquet will also perform some column-level compression, so not all columns with the same type will take up the same space on disk.\n",
    "\n",
    "Below, we inspect the row and column group metadata to show the compressed size of the fields on disk. The last column, `percent`, show the percent of total size taken up by the column.\n",
    "\n",
    "You *can* use this to inform which columns you keep when importing a catalog into hipscat format. e.g. if some columns are less useful for your science, and take up a lot of space, maybe leave them out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dcf152f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Y</td>\n",
       "      <td>48237</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RA</td>\n",
       "      <td>48237</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>X</td>\n",
       "      <td>48237</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GLON</td>\n",
       "      <td>48237</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GLAT</td>\n",
       "      <td>48237</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>W2MCOR</td>\n",
       "      <td>129</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>SSO_FLG</td>\n",
       "      <td>99</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>NA</td>\n",
       "      <td>99</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SCAN_ID</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>MOON_MASKED</td>\n",
       "      <td>65</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            name   size percent\n",
       "139            Y  48237     2.5\n",
       "4             RA  48237     2.5\n",
       "138            X  48237     2.5\n",
       "9           GLON  48237     2.5\n",
       "10          GLAT  48237     2.5\n",
       "..           ...    ...     ...\n",
       "52        W2MCOR    129     0.0\n",
       "113      SSO_FLG     99     0.0\n",
       "39            NA     99     0.0\n",
       "2        SCAN_ID     77     0.0\n",
       "110  MOON_MASKED     65     0.0\n",
       "\n",
       "[143 rows x 3 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "num_cols = parquet_file.metadata.num_columns\n",
    "num_row_groups = parquet_file.metadata.num_row_groups\n",
    "sizes = np.zeros(num_cols)\n",
    "\n",
    "for rg in range(num_row_groups):\n",
    "    for col in range (num_cols):\n",
    "        sizes[col] += parquet_file.metadata.row_group(rg).column(col).total_compressed_size\n",
    "\n",
    "## This is just an attempt at pretty formatting\n",
    "percents = [f\"{s/sizes.sum()*100:.1f}\" for s in sizes]\n",
    "pd.DataFrame({\"name\":parquet_file.schema.names, \"size\":sizes.astype(int), \"percent\": percents}).sort_values(\"size\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01780b91",
   "metadata": {},
   "source": [
    "You can also use this opportunity to create a schema-only parquet file to ensure that your final parquet files use the appropriate field types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "59c65fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_only_file = \"/data3/epyc/data3/hipscat/tmp/neowise_schema.parquet\"\n",
    "pq.write_table(pa.Table.from_pandas(data_frame).schema.empty_table(), where=schema_only_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Melissa LSDB",
   "language": "python",
   "name": "lsd2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
