{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced285da",
   "metadata": {},
   "source": [
    "# Approaching \"Unsupported cast\" errors\n",
    "\n",
    "**Background**\n",
    "\n",
    "Occasionally, an import will throw errors like the following:\n",
    "\n",
    "```\n",
    "Key:       4_2666\n",
    "Function:  reduce_pixel_shards\n",
    "args:      ()\n",
    "kwargs:    {...}\n",
    "Exception: \"ArrowNotImplementedError('Unsupported cast from string to null using function cast_null')\"\n",
    "```\n",
    "\n",
    "We've observed that this is due to the way that PyArrow encodes types in parquet files.\n",
    "\n",
    "At the reduce stage, we're combining several intermediate parquet files for a single spatial tile into the final parquet file. It's possible at this stage that some files will contain only empty (null) values in a column that we expect to be a string field.\n",
    "\n",
    "e.g. \n",
    "\n",
    "File1\n",
    "\n",
    "| int_field | string_field | float_field |\n",
    "| --------- | ------------ | ----------  |\n",
    "|         5 |      <empty> |         3.4 |\n",
    "|         8 |      <empty> |         3.8 |\n",
    "\n",
    "which will have a schema like:\n",
    "       \n",
    "    optional int64 field_id=-1 int_field;\n",
    "    optional int32 field_id=-1 string_field **(Null)**;\n",
    "    optional double field_id=-1 float_field;\n",
    "    \n",
    "File2\n",
    "    \n",
    "| int_field | string_field | float_field |\n",
    "| --------- |------------- | ----------- |\n",
    "|         6 |      hello   |         4.1 |\n",
    "|         7 |      <empty> |         3.9 |\n",
    "\n",
    "will have a schema like:\n",
    "\n",
    "    optional int64 field_id=-1 int_field;\n",
    "    optional binary field_id=-1 string_field (String);\n",
    "    optional double field_id=-1 float_field;\n",
    "\n",
    "When we try to merge these files together, the parquet engine does not want to perform a cast between these types, and throws an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538d286",
   "metadata": {},
   "source": [
    "**Objective**\n",
    "\n",
    "In this notebook, we'll look at an approach to resolving this issue: generating a single standard parquet schema file to use when writing the dataset.\n",
    "\n",
    "**Read a single input file**\n",
    "\n",
    "Start by reading a single input file as a pandas dataframe. You can reuse the InputReader class that you've instantiated for the import pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f1b6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from hipscat_import.catalog.file_readers import CsvReader\n",
    "\n",
    "input_file=\"/data3/epyc/data3/hipscat/raw/allwise_raw/wise-allwise-cat-part05\"\n",
    "\n",
    "## This input CSV file requires header and type data from another source.\n",
    "type_frame = pd.read_csv(\"/astro/users/mmd11/git/hipscripts/epyc/allwise/allwise_types.csv\")\n",
    "type_names = type_frame[\"name\"].values.tolist()\n",
    "type_map = dict(zip(type_frame[\"name\"], type_frame[\"type\"]))\n",
    "\n",
    "file_reader = CsvReader(\n",
    "                    header=None,\n",
    "                    separator=\"|\",\n",
    "                    column_names=type_frame[\"name\"].values.tolist(),\n",
    "                    type_map=type_map,\n",
    "                    chunksize=5\n",
    "                )\n",
    "\n",
    "data_frame = next(file_reader.read(input_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04409c42",
   "metadata": {},
   "source": [
    "Now that we have the typed data from the input file, write out only the column-level schema to a new file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dedce09d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m schema_only_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data3/epyc/data3/hipscat/tmp/allwise_schema.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpq\u001b[49m\u001b[38;5;241m.\u001b[39mwrite_table(pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_pandas(data_frame)\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mempty_table(), where\u001b[38;5;241m=\u001b[39mschema_only_file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pq' is not defined"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "schema_only_file = \"/data3/epyc/data3/hipscat/tmp/allwise_schema.parquet\"\n",
    "pq.write_table(pa.Table.from_pandas(data_frame).schema.empty_table(), where=schema_only_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Melissa LSDB",
   "language": "python",
   "name": "lsd2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
