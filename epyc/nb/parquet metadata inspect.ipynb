{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae182e04",
   "metadata": {},
   "source": [
    "# Parquet METADATA file inspection\n",
    "\n",
    "In this notebook, we look at methods to explore the contents of a `_metadata` parquet file.\n",
    "\n",
    "\n",
    "**Author**: Melissa DeLucchi (delucchi@andrew.cmu.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee8d8c",
   "metadata": {},
   "source": [
    "First, instantiate your parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "574ea901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "## You're gonna want to change this file name!!\n",
    "\n",
    "file_name = \"/data3/epyc/data3/hipscat/catalogs/ztf_apr18/ztf_dr14/_metadata\"\n",
    "parquet_file = pq.ParquetFile(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "432affe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow._parquet.FileMetaData object at 0x7f72a1b5bc20>\n",
      "  created_by: parquet-cpp-arrow version 9.0.0\n",
      "  num_columns: 16\n",
      "  num_rows: 1234677579\n",
      "  num_row_groups: 2360\n",
      "  format_version: 2.6\n",
      "  serialized_size: 5395265\n",
      "16\n",
      "2360\n"
     ]
    }
   ],
   "source": [
    "print(parquet_file.metadata)\n",
    "print(parquet_file.metadata.num_columns)\n",
    "cols = parquet_file.metadata.num_columns\n",
    "print(parquet_file.metadata.num_row_groups)\n",
    "row_groups = parquet_file.metadata.num_row_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4fbc6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               name          size    percent\n",
      "10       mean_mag_r  9.315607e+09  14.585721\n",
      "0         ps1_objid  8.908720e+09  13.948646\n",
      "15   _hipscat_index  7.317717e+09  11.457565\n",
      "2               dec  6.963166e+09  10.902435\n",
      "1                ra  6.296127e+09   9.858032\n",
      "9        mean_mag_g  6.225676e+09   9.747724\n",
      "11       mean_mag_i  4.663170e+09   7.301263\n",
      "5   ps1_iMeanPSFMag  3.603279e+09   5.641760\n",
      "4   ps1_rMeanPSFMag  3.586042e+09   5.614772\n",
      "3   ps1_gMeanPSFMag  3.256742e+09   5.099177\n",
      "7            nobs_r  1.672007e+09   2.617910\n",
      "6            nobs_g  1.406545e+09   2.202268\n",
      "8            nobs_i  6.526143e+08   1.021817\n",
      "12           Norder  1.935050e+05   0.000303\n",
      "13              Dir  1.935050e+05   0.000303\n",
      "14             Npix  1.935050e+05   0.000303\n"
     ]
    }
   ],
   "source": [
    "## Size on disk of each column\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sizes = np.zeros(cols)\n",
    "\n",
    "for rg in range(row_groups):\n",
    "    for col in range (cols):\n",
    "        sizes[col] += parquet_file.metadata.row_group(rg).column(col).total_compressed_size\n",
    "        \n",
    "frame = pd.DataFrame({\"name\":parquet_file.schema.names, \"size\":sizes})\n",
    "frame['percent'] = frame['size'] / frame['size'].sum() *100\n",
    "frame = frame.sort_values(\"size\", ascending=False)\n",
    "# frame.to_csv(\"/data3/epyc/data3/hipscat/raw/pan_starrs/sample_detection_size.csv\")\n",
    "print(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66a25a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow._parquet.RowGroupMetaData object at 0x7f72a1b48d60>\n",
      "  num_columns: 16\n",
      "  num_rows: 243669\n",
      "  total_byte_size: 17237119\n"
     ]
    }
   ],
   "source": [
    "row = parquet_file.metadata.row_group(1001)\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d04bfb",
   "metadata": {},
   "source": [
    "maybe also look at the partition info, to check that they match size?\n",
    "import pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "187d1cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2352\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_name = \"/data3/epyc/data3/hipscat/catalogs/ztf_apr18/ztf_dr14/partition_info.csv\"\n",
    "partition_info = pd.read_csv(file_name)\n",
    "print(len(partition_info))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc14f8c7",
   "metadata": {},
   "source": [
    "## Schema\n",
    "\n",
    "Show all the columns and their stored type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70870084",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parquet_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mparquet_file\u001b[49m\u001b[38;5;241m.\u001b[39mschema)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parquet_file' is not defined"
     ]
    }
   ],
   "source": [
    "print(parquet_file.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7ee489",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "Types are helpful, but nothing beats seeing the data. The snippet below will load the parquet file and trim to just the first two rows. By transposing the result, the columns become rows and the whole thing is easier to skim through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "842ccfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          mag  magerr\n",
      "173533132198219908  16.389799     174\n",
      "173533132243349711  18.985800     612\n",
      "173533132500747980  17.901400     313\n",
      "173533132520209275  20.043100    1380\n",
      "173533132623167299  20.023899    1359\n",
      "...                       ...     ...\n",
      "173993134962710289  20.286100    1340\n",
      "173993134962710289  20.055901    1348\n",
      "173993134962710289  20.153500    1894\n",
      "173993134964711814  20.546499    2647\n",
      "173993134964711814  20.702600    1870\n",
      "\n",
      "[301348 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data_frame = pd.read_parquet(file_name, engine=\"pyarrow\")\n",
    "# print(data_frame.head(1).transpose())\n",
    "# print(data_frame.loc[[929796]].transpose())\n",
    "\n",
    "# data_frame.loc[[929796]].to_parquet(\"/astro/users/mmd11/data/ztf_row.parquet\")\n",
    "\n",
    "# print(data_frame.head(10))\n",
    "mags = data_frame[[\"mag\", \"magerr\"]]\n",
    "print(mags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a3a293e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929796\n",
      "found nothing\n"
     ]
    }
   ],
   "source": [
    "# print(len(data_frame['mjd_r'][0]))\n",
    "\n",
    "for index in range(len(data_frame)):\n",
    "    if len(data_frame[\"mjd_r\"][index]) and len(data_frame[\"mjd_g\"][index]) and len(data_frame[\"mjd_i\"][index]):\n",
    "        print(index)\n",
    "        break\n",
    "#     print(f'{index}, {len(data_frame[\"mjd_r\"][index])}, {len(data_frame[\"mjd_g\"][index])}, {len(data_frame[\"mjd_i\"][index])}')\n",
    "print(\"found nothing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685f04d3",
   "metadata": {},
   "source": [
    "I often want to know what are the values that a field can take, if there are only a handful (e.g. for essentially an ENUM type).\n",
    "\n",
    "The following will fetch all the values from a single column, find all the unique values, and spit out some of them. There's definitely a cuter pandas way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0d51e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values: 301348\n",
      "Number of unique values: 5\n",
      "Sample of unique values: [3, 14, 21, 53, 24]\n"
     ]
    }
   ],
   "source": [
    "## you might need to change the id column\n",
    "id_column = 'rcidin'\n",
    "\n",
    "assert id_column in data_frame.columns\n",
    "ids = data_frame[id_column].tolist()\n",
    "print(f'Number of values: {len(ids)}')\n",
    "set_ids = [*set(ids)]\n",
    "print(f'Number of unique values: {len(set_ids)}')\n",
    "print(f'Sample of unique values: {set_ids[0:5]}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4fa52a39",
   "metadata": {},
   "source": [
    "Or just find the min/max of a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "008a2aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                       131071\n",
       "_hipscat_id    9778440568160911517\n",
       "ps1_objid        75240618853871027\n",
       "ra                       63.263256\n",
       "dec                     -27.299459\n",
       "catflags                      4100\n",
       "fieldID                       1248\n",
       "mag                      21.929874\n",
       "maggerr                   0.371297\n",
       "mjd                    59627.16843\n",
       "rcID                            47\n",
       "band                             r\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c1454f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                            0\n",
       "_hipscat_id    9777314802224332800\n",
       "ps1_objid        72000618713066946\n",
       "ra                       60.475616\n",
       "dec                     -29.994318\n",
       "catflags                    -32768\n",
       "fieldID                        253\n",
       "mag                      11.443596\n",
       "maggerr                   0.003859\n",
       "mjd                    58363.50343\n",
       "rcID                             0\n",
       "band                             g\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.min(axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Melissa LSDB",
   "language": "python",
   "name": "lsd2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
